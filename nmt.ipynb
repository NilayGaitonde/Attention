{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader,Dataset,TensorDataset,RandomSampler\n",
    "import time\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "class Language:\n",
    "    def __init__(self,name):\n",
    "        self.wordcount = {}\n",
    "        self.word2index = {}\n",
    "        self.name = name\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.num_words = 2\n",
    "\n",
    "    def addWord(self,word):\n",
    "        if word not in self.wordcount.keys():\n",
    "            self.num_words += 1\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.wordcount[word] = 1\n",
    "        else:\n",
    "            self.wordcount[word] += 1\n",
    "\n",
    "    def addSentence(self,sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToASCII(unicode):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD',unicode)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def normaliseText(text):\n",
    "    # regex to remove punctuations\n",
    "    text = re.sub(r\"([.!?])\",r\" \\1\",text)\n",
    "    text = unicodeToASCII(text.lower().strip())\n",
    "    text = re.sub(r\"[^a-zA-Z!?]+\", r\" \", text)\n",
    "    return text.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(reverse=False):\n",
    "    lines = open(\"fra.txt\",\"r\").read().splitlines()\n",
    "    pairs = [[normaliseText(s) for s in l.split('\\t')] for l in lines]\n",
    "    print(f\"{len(pairs)}\")\n",
    "    # pairs = filterPairs(pairs)\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        french = Language(\"French\")\n",
    "        english = Language(\"English\")\n",
    "    else:\n",
    "        english = Language(\"English\")\n",
    "        french = Language(\"French\")\n",
    "    return english,french,pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232736\n",
      "No. of Sentences: 232736\n",
      "No of english words: 16033\n",
      "No of french words: 25684\n"
     ]
    }
   ],
   "source": [
    "english,french,pairs = readData(False)\n",
    "print(f\"No. of Sentences: {len(pairs)}\")\n",
    "for pair in pairs:\n",
    "    english.addSentence(pair[0])\n",
    "    french.addSentence(pair[1])\n",
    "print(f\"No of english words: {english.num_words}\")\n",
    "print(f\"No of french words: {french.num_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs[:len(pairs)//2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,dropout_p=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size,hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size,hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "    def forward(self,X):\n",
    "        embed = self.dropout(self.embedding(X))\n",
    "        output, final_hidden, final_cell = self.lstm(embed)\n",
    "        return output, final_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self,hidden_size,output_size,dropout_p=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(hidden_size,hidden_size,batch_first=True)\n",
    "        self.embedding = nn.Embedding(output_size,hidden_size)\n",
    "        self.ff = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward_step(self, inputs, hidden,target_tensor=None):\n",
    "        out = self.embedding(inputs)\n",
    "        out = nn.ReLU(out)\n",
    "\n",
    "        output, final_hidden,final_cell = self.lstm(out, hidden)\n",
    "        output = self.ff(output)\n",
    "        return output, final_hidden\n",
    "\n",
    "    def forward(self, encoder_output,encoder_hidden):\n",
    "        batch_size = encoder_output.shape(0)\n",
    "        decoder_input = torch.ones(batch_size,1,dtype=torch.long,device=device).fill_(SOS_token)\n",
    "        decoder_outputs = []\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "        decoder_outputs.append(decoder_outputs)\n",
    "        _, topi = decoder_output.topk(1) # this will return the value of the highest probabilty and the index on which that probability is present\n",
    "        decoder_input = topi.squeeze(-1).detach()\n",
    "        while topi.item() != EOS_token:\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_outputs)\n",
    "            _,topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze(-1).detach()\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceToIndex(sentence:str,lang:Language):\n",
    "    indices = [SOS_token]\n",
    "    indices.extend([lang.word2index[normaliseText(word)] for word in sentence.split(\" \")])\n",
    "    indices.append(EOS_token)\n",
    "    return indices\n",
    "\n",
    "def tensorFromSentence(sentence:str,lang:Language):\n",
    "    indexes = [SOS_token]\n",
    "    indexes.extend(sentenceToIndex(sentence,lang))\n",
    "    indexes.append(EOS_token)\n",
    "    tensor_indexes = torch.tensor(indexes,dtype=torch.long,device=device).view(1,-1)\n",
    "    return tensor_indexes\n",
    "\n",
    "def tensorFromPair(pairs:list,lang1:Language,lang2:Language):\n",
    "    index_1 = tensorFromSentence(pairs[0],lang1)\n",
    "    index_2 = tensorFromSentence(pairs[1],lang2)\n",
    "    return (index_1,index_2)\n",
    "\n",
    "def getData(batch_size):\n",
    "    # english,french,pairs = readData()\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n,n),dtype=np.int32)\n",
    "    output_ids = np.zeros((n,n),dtype=np.int32)\n",
    "\n",
    "    for idx, (input_sent, output_sent,_) in enumerate(pairs):\n",
    "        # print(input_sent,english.name)\n",
    "        # print(output_sent,french.name)\n",
    "        input_index = sentenceToIndex(input_sent,english)\n",
    "        output_index = sentenceToIndex(output_sent,french)\n",
    "        input_ids[idx,:len(input_index)] = input_index\n",
    "        output_ids[idx,:len(output_index)] = output_index\n",
    "    train_data = TensorDataset(torch.tensor(input_ids,dtype=torch.long,device=device),torch.tensor(output_sent,dtype=torch.long,device=device))\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size,device=device)\n",
    "    return english,french,train_dataloader\n",
    "    # return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_data, encoder, decoder, encoder_opt, decoder_opt, criterion):\n",
    "    total_loss=0\n",
    "    for data in train_data:\n",
    "        input_tensor,target_tensor = data\n",
    "\n",
    "        encoder_opt.zero_grad()\n",
    "        decoder_opt.zero_grad()\n",
    "\n",
    "\n",
    "        encoder_output, final_hidden = encoder(input_tensor)\n",
    "        decoder_output, decoder_hidden = decoder(encoder_output,final_hidden)\n",
    "        loss = criterion(decoder_output.view(-1,decoder_output.size(-1)),target_tensor.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_opt.step()\n",
    "        decoder_opt.step()\n",
    "\n",
    "        total_loss+=loss.item()\n",
    "\n",
    "    return total_loss/len(train_data)\n",
    "\n",
    "def training(train_dataloader,encoder,decoder,epochs,lr=1e-3,print_interval=100):\n",
    "    start = time.time()\n",
    "    losses = list()\n",
    "    encoder_opt = torch.optim.Adam(encoder.parameters(),lr=lr)\n",
    "    decoder_opt = torch.optim.Adam(decoder.parameters(),lr=lr)\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        loss = train_epoch(train_dataloader,encoder,decoder,encoder_opt,decoder_opt,criterion)\n",
    "        losses.append(loss)\n",
    "        if epoch%print_interval == 0:\n",
    "            print(f\"epoch: {epoch:.3d}: loss = {loss:.7d} \\tavg_loss = {sum(losses)/epoch}({time.time()-start} secs)\")\n",
    "    return losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "english,french,train_dataloader = getData(batch_size)\n",
    "print(f\"No. of Sentences: {len(pairs)}\")\n",
    "for pair in pairs:\n",
    "    english.addSentence(pair[0])\n",
    "    french.addSentence(pair[1])\n",
    "print(f\"No of english words: {english.num_words}\")\n",
    "print(f\"No of french words: {french.num_words}\")\n",
    "\n",
    "encoder = EncoderLSTM(english.num_words,hidden_size)\n",
    "decoder = DecoderLSTM(hidden_size,french.num_words)\n",
    "\n",
    "losses = training(train_dataloader,encoder,decoder,epochs,print_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
