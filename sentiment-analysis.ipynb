{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset,TensorDataset,RandomSampler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is available at [Link](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontract(sentence):\n",
    "    sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"\\'s\", \" is\", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "    sentence = re.sub(r\"\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"\\'m\", \" am\", sentence)\n",
    "    return sentence\n",
    "\n",
    "def cleanPunc(sentence): \n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = \"\"\n",
    "    for word in sentence.split():\n",
    "        alpha_word = re.sub('[^a-z A-Z]+', '', word)\n",
    "        alpha_sent += alpha_word\n",
    "        alpha_sent += \" \"\n",
    "    alpha_sent = alpha_sent.strip()\n",
    "    return alpha_sent\n",
    "\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\"\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stopwords) + \")\\\\W\", re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>568450</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A28KG5XORO54AY</td>\n",
       "      <td>Lettie D. Carter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1299628800</td>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>568451</td>\n",
       "      <td>B003S1WTCU</td>\n",
       "      <td>A3I8AFVPEE8KI5</td>\n",
       "      <td>R. Sawyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1331251200</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>568452</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A121AA1GQV751Z</td>\n",
       "      <td>pksd \"pk_007\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1329782400</td>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>568453</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A3IBEVCTXKNOH</td>\n",
       "      <td>Kathy A. Welch \"katwel\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1331596800</td>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>568454</td>\n",
       "      <td>B001LR2CU2</td>\n",
       "      <td>A3LGQPJCZVL9UC</td>\n",
       "      <td>srfell17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568454 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                      ProfileName  \\\n",
       "0            1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1            2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2            3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3            4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4            5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "...        ...         ...             ...                              ...   \n",
       "568449  568450  B001EO7N10  A28KG5XORO54AY                 Lettie D. Carter   \n",
       "568450  568451  B003S1WTCU  A3I8AFVPEE8KI5                        R. Sawyer   \n",
       "568451  568452  B004I613EE  A121AA1GQV751Z                    pksd \"pk_007\"   \n",
       "568452  568453  B004I613EE   A3IBEVCTXKNOH          Kathy A. Welch \"katwel\"   \n",
       "568453  568454  B001LR2CU2  A3LGQPJCZVL9UC                         srfell17   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                          1                       1      5  1303862400   \n",
       "1                          0                       0      1  1346976000   \n",
       "2                          1                       1      4  1219017600   \n",
       "3                          3                       3      2  1307923200   \n",
       "4                          0                       0      5  1350777600   \n",
       "...                      ...                     ...    ...         ...   \n",
       "568449                     0                       0      5  1299628800   \n",
       "568450                     0                       0      2  1331251200   \n",
       "568451                     2                       2      5  1329782400   \n",
       "568452                     1                       1      5  1331596800   \n",
       "568453                     0                       0      5  1338422400   \n",
       "\n",
       "                                   Summary  \\\n",
       "0                    Good Quality Dog Food   \n",
       "1                        Not as Advertised   \n",
       "2                    \"Delight\" says it all   \n",
       "3                           Cough Medicine   \n",
       "4                              Great taffy   \n",
       "...                                    ...   \n",
       "568449                 Will not do without   \n",
       "568450                        disappointed   \n",
       "568451            Perfect for our maltipoo   \n",
       "568452  Favorite Training and reward treat   \n",
       "568453                         Great Honey   \n",
       "\n",
       "                                                     Text  \n",
       "0       I have bought several of the Vitality canned d...  \n",
       "1       Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2       This is a confection that has been around a fe...  \n",
       "3       If you are looking for the secret ingredient i...  \n",
       "4       Great taffy at a great price.  There was a wid...  \n",
       "...                                                   ...  \n",
       "568449  Great for sesame chicken..this is a good if no...  \n",
       "568450  I'm disappointed with the flavor. The chocolat...  \n",
       "568451  These stars are small, so you can give 10-15 o...  \n",
       "568452  These are the BEST treats for training and rew...  \n",
       "568453  I am very satisfied ,product is as advertised,...  \n",
       "\n",
       "[568454 rows x 10 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/reviews.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241812\n"
     ]
    }
   ],
   "source": [
    "X = data[data['Text'].apply(lambda x: len(x.strip().split(\" \"))<50)]['Text']\n",
    "print(len(X)) # 241812\n",
    "max_len = max([len(x) for x in X])\n",
    "y = data.loc[data[data['Text'].apply(lambda x: len(x.split(\" \"))<50)].index]['Score'].apply(lambda x: int(x>=3)).values\n",
    "# y = F.one_hot(torch.tensor(y),num_classes=2).to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliseText(X):\n",
    "    X = X.str.lower()\n",
    "    X = X.apply(decontract)\n",
    "    X = X.apply(cleanPunc)\n",
    "    X = X.apply(keepAlpha)\n",
    "    X = X.apply(removeStopWords)\n",
    "    X = X.apply(lambda x: re.sub(r'(\\w)(\\1{2,})', r'\\1',x)) \n",
    "    return X\n",
    "X = normaliseText(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Words:\n",
    "    def __init__(self):\n",
    "        self.words2index={}\n",
    "        self.index2words={}\n",
    "        self.word_count = {}\n",
    "        self.n_words = 1\n",
    "    def addWord(self,word:str):\n",
    "        if word not in self.word_count.keys():\n",
    "            self.words2index[word] = self.n_words\n",
    "            self.index2words[self.n_words] = word\n",
    "            self.word_count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word_count[word]+=1\n",
    "\n",
    "    def addSentence(self,sentence:str):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)\n",
    "    \n",
    "    def encode(self,sentence:str):\n",
    "        encoded_rep = list()\n",
    "        for word in sentence.split(\" \"):\n",
    "            try:\n",
    "                encoded_word = self.words2index[word.lower()]\n",
    "            except KeyError as e:\n",
    "                print(f\"Adding {word} and encoding\")\n",
    "                self.addWord(word.lower())\n",
    "                encoded_word = self.words2index[word.lower()]\n",
    "            encoded_rep.append(encoded_word)\n",
    "        return encoded_rep\n",
    "    \n",
    "    def decode(self,sentence:str):\n",
    "        decoded_rep = list()\n",
    "        for encoded_rep in sentence.split(\" \"):\n",
    "            if encoded_rep == 100000:\n",
    "                continue\n",
    "            decoded_word = self.index2words[encoded_rep]\n",
    "            decoded_rep.append(decoded_word)\n",
    "        return decoded_rep\n",
    "words = Words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of words: 54807\n"
     ]
    }
   ],
   "source": [
    "X.apply(words.addSentence)\n",
    "print(f\"Num of words: {words.n_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max(X.apply(lambda x:len(x.strip().split(\" \"))))\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(text:list):\n",
    "    return text + [0] * (max_len-len(text))\n",
    "def prepareData(data:list):\n",
    "    input_tensor = torch.zeros((len(data),max_len),dtype=torch.long)\n",
    "    for i,x in enumerate(data):\n",
    "        x = words.encode(x)\n",
    "        x = pad_sequences(x)\n",
    "        input_tensor[i] = torch.tensor(x,dtype=torch.long)\n",
    "    return input_tensor\n",
    "input_tensor = prepareData(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(input_tensor, y, \n",
    "                                                    test_size=0.2, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BUFFER_SIZE = len(X_train)\n",
    "VAL_BUFFER_SIZE = len(X_val)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "TRAIN_N_BATCH = TRAIN_BUFFER_SIZE // BATCH_SIZE\n",
    "VAL_N_BATCH = VAL_BUFFER_SIZE // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reviews(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Reviews(X_train, y_train)\n",
    "val_dataset = Reviews(X_val, y_val)\n",
    "\n",
    "train_dataset = DataLoader(train_dataset, batch_size = BATCH_SIZE,\n",
    "                          drop_last=True, shuffle=True)\n",
    "val_dataset = DataLoader(val_dataset, batch_size = BATCH_SIZE,\n",
    "                          drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,head_size, n_embed):\n",
    "        super().__init__()\n",
    "        self.keys = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.queries = nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.values = nn.Linear(n_embed,head_size,bias=False)\n",
    "    \n",
    "    def __call__(self,X):\n",
    "        # X = (64, 55, 500) \n",
    "        K = self.keys(X) # (64,55,500) @ (500,16) -> (64, 55, 16)\n",
    "        Q = self.queries(X) # (64,55,500) @ (500,16) -> (64, 55, 16)\n",
    "        V = self.values(X) # (64,55,500) @ (500,16) -> (64, 55, 16)\n",
    "\n",
    "        e = Q @ K.transpose(1,2) / (K.shape[-1]**0.5) # (64, 55, 16) @ (64,16,55) -> (64,55,55) \n",
    "        weights = e.masked_fill((torch.tril(torch.ones(X.shape[1], X.shape[1]))==0).to(device),-torch.inf) # (64,55,55)\n",
    "        weights = F.softmax(weights) \n",
    "        out = weights @ V # (64,55,55) @ (64,55,16) -> (64,55,16)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 500\n",
    "target_size = 2\n",
    "vocab_size = words.n_words\n",
    "PATH = './model/model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,embedding_dim,vocab_size,hidden_size,target_size,head_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size,embedding_dim=embedding_dim,padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim,hidden_size,batch_first=True)\n",
    "        self.attention = Attention(head_size=head_size,n_embed=hidden_size).to(device)\n",
    "        self.batchnorm = nn.BatchNorm1d(max_len*head_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.linear = nn.Linear(max_len*head_size,target_size,bias=True)\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.ones((1,self.batch_size,self.hidden_size),device=device),torch.ones((1,self.batch_size,self.hidden_size),device=device))\n",
    "    def forward(self,X):\n",
    "        # X = (64,55)\n",
    "        x = self.embed(X) # (64,55) -> (64,55,256)\n",
    "        (h_n,c_n) = self.init_hidden()\n",
    "        x,(hidden,cell) = self.lstm(x,(h_n,c_n)) # (64,55,256) -> (64,55,500)\n",
    "        output = self.attention(x) # (64,55,500) -> (64,55,16)\n",
    "        output = output.view(output.size(0),-1) # (64,55,16) -> (64,880) # [64,55*16] \n",
    "        output = self.batchnorm(output) # (64,800) -> (64,800)\n",
    "        output = self.tanh(output)\n",
    "        output = self.linear(output) # (64,880) -> (64,2)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embed): Embedding(54807, 256, padding_idx=0)\n",
      "  (lstm): LSTM(256, 500, batch_first=True)\n",
      "  (attention): Attention(\n",
      "    (keys): Linear(in_features=500, out_features=16, bias=False)\n",
      "    (queries): Linear(in_features=500, out_features=16, bias=False)\n",
      "    (values): Linear(in_features=500, out_features=16, bias=False)\n",
      "  )\n",
      "  (batchnorm): BatchNorm1d(880, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (tanh): Tanh()\n",
      "  (linear): Linear(in_features=880, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model(embedding_dim,vocab_size,units,target_size,16).to(device=device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(targets, predictions):\n",
    "    correct = (targets.argmax(1).to(device) == predictions.argmax(1).to(device)).sum()\n",
    "    accuracy = 100. * correct / len(predictions)\n",
    "    return accuracy\n",
    "\n",
    "def save_model(EPOCH,model,optim,LOSS,PATH):\n",
    "    torch.save({\n",
    "            'epoch': EPOCH,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'loss': LOSS,\n",
    "            }, PATH)\n",
    "def training(train_data,val_data,criterion,optim,epochs,model,batch_interval=100,epoch_interval=100):\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(\"training\")\n",
    "        train_total_accuracy, val_total_accuracy = 0,0\n",
    "        train_total_count, val_total_count = 0,0\n",
    "        for (batch,(X,y)) in enumerate(train_data):\n",
    "            y = y.to(device)\n",
    "            logits = model(X.to(device))\n",
    "            loss=criterion(logits,y)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_total_accuracy += (logits.argmax(1) == y).sum().item()\n",
    "            train_total_count += y.size(0)\n",
    "            if batch % batch_interval == 0:\n",
    "                save_model(epoch,model,optim,loss,PATH)\n",
    "                print(f\"{epoch:3d}: [{batch:4d}/{len(train_data):4d}] {(train_total_accuracy/train_total_count):.4f}\")\n",
    "        model.eval()\n",
    "        for (batch,(X,y)) in enumerate(val_data):\n",
    "            y = y.to(device)\n",
    "            logits = model(X.to(device))\n",
    "            val_total_accuracy += (logits.argmax(1) == y).sum().item()\n",
    "            val_total_count += y.size(0)\n",
    "            if batch % batch_interval == 0:\n",
    "                print(f\"{epoch:3d}: [{batch:4d}/{len(val_data):4d}] {(val_total_accuracy/val_total_count):.4f}\")\n",
    "        print(f\"{batch}/{len(val_data)} validation\")\n",
    "        if epoch % epoch_interval == 0:\n",
    "            print(f\"epoch:{epoch}/{epochs} train_accuracy:{train_total_accuracy/train_total_count} | val_accuracy:{val_total_accuracy/val_total_count}\")\n",
    "    print(f\"{time.time()-start} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b4/9cg_tkrn3256x3hcybpxqnpw0000gn/T/ipykernel_15007/257930496.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  weights = F.softmax(weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: [   0/3022] 0.3906\n",
      "  0: [ 100/3022] 0.8696\n",
      "  0: [ 200/3022] 0.8755\n",
      "  0: [ 300/3022] 0.8770\n",
      "  0: [ 400/3022] 0.8771\n",
      "  0: [ 500/3022] 0.8768\n",
      "  0: [ 600/3022] 0.8768\n",
      "  0: [ 700/3022] 0.8766\n",
      "  0: [ 800/3022] 0.8775\n",
      "  0: [ 900/3022] 0.8774\n",
      "  0: [1000/3022] 0.8774\n",
      "  0: [1100/3022] 0.8769\n",
      "  0: [1200/3022] 0.8770\n",
      "  0: [1300/3022] 0.8772\n",
      "  0: [1400/3022] 0.8775\n",
      "  0: [1500/3022] 0.8779\n",
      "  0: [1600/3022] 0.8776\n",
      "  0: [1700/3022] 0.8776\n",
      "  0: [1800/3022] 0.8778\n",
      "  0: [1900/3022] 0.8778\n",
      "  0: [2000/3022] 0.8778\n",
      "  0: [2100/3022] 0.8775\n",
      "  0: [2200/3022] 0.8776\n",
      "  0: [2300/3022] 0.8776\n",
      "  0: [2400/3022] 0.8776\n",
      "  0: [2500/3022] 0.8774\n",
      "  0: [2600/3022] 0.8778\n",
      "  0: [2700/3022] 0.8779\n",
      "  0: [2800/3022] 0.8780\n",
      "  0: [2900/3022] 0.8779\n",
      "  0: [3000/3022] 0.8780\n",
      "  0: [   0/ 755] 0.8906\n",
      "  0: [ 100/ 755] 0.8820\n",
      "  0: [ 200/ 755] 0.8827\n",
      "  0: [ 300/ 755] 0.8825\n",
      "  0: [ 400/ 755] 0.8825\n",
      "  0: [ 500/ 755] 0.8806\n",
      "  0: [ 600/ 755] 0.8790\n",
      "  0: [ 700/ 755] 0.8805\n",
      "754/755 validation\n",
      "epoch:0/5 train_accuracy:0.8779781601588352 | val_accuracy:0.8805256622516556\n",
      "training\n",
      "  1: [   0/3022] 0.8594\n",
      "  1: [ 100/3022] 0.8741\n",
      "  1: [ 200/3022] 0.8774\n",
      "  1: [ 300/3022] 0.8788\n",
      "  1: [ 400/3022] 0.8789\n",
      "  1: [ 500/3022] 0.8786\n",
      "  1: [ 600/3022] 0.8779\n",
      "  1: [ 700/3022] 0.8782\n",
      "  1: [ 800/3022] 0.8789\n",
      "  1: [ 900/3022] 0.8788\n",
      "  1: [1000/3022] 0.8787\n",
      "  1: [1100/3022] 0.8789\n",
      "  1: [1200/3022] 0.8790\n",
      "  1: [1300/3022] 0.8786\n",
      "  1: [1400/3022] 0.8785\n",
      "  1: [1500/3022] 0.8783\n",
      "  1: [1600/3022] 0.8783\n",
      "  1: [1700/3022] 0.8781\n",
      "  1: [1800/3022] 0.8782\n",
      "  1: [1900/3022] 0.8781\n",
      "  1: [2000/3022] 0.8780\n",
      "  1: [2100/3022] 0.8778\n",
      "  1: [2200/3022] 0.8778\n",
      "  1: [2300/3022] 0.8775\n",
      "  1: [2400/3022] 0.8777\n",
      "  1: [2500/3022] 0.8778\n",
      "  1: [2600/3022] 0.8779\n",
      "  1: [2700/3022] 0.8781\n",
      "  1: [2800/3022] 0.8779\n",
      "  1: [2900/3022] 0.8778\n",
      "  1: [3000/3022] 0.8781\n",
      "  1: [   0/ 755] 0.8438\n",
      "  1: [ 100/ 755] 0.8855\n",
      "  1: [ 200/ 755] 0.8818\n",
      "  1: [ 300/ 755] 0.8827\n",
      "  1: [ 400/ 755] 0.8813\n",
      "  1: [ 500/ 755] 0.8825\n",
      "  1: [ 600/ 755] 0.8827\n",
      "  1: [ 700/ 755] 0.8816\n",
      "754/755 validation\n",
      "epoch:1/5 train_accuracy:0.8781332726671078 | val_accuracy:0.8806291390728477\n",
      "training\n",
      "  2: [   0/3022] 0.8750\n",
      "  2: [ 100/3022] 0.8728\n",
      "  2: [ 200/3022] 0.8745\n",
      "  2: [ 300/3022] 0.8774\n",
      "  2: [ 400/3022] 0.8780\n",
      "  2: [ 500/3022] 0.8789\n",
      "  2: [ 600/3022] 0.8785\n",
      "  2: [ 700/3022] 0.8781\n",
      "  2: [ 800/3022] 0.8780\n",
      "  2: [ 900/3022] 0.8774\n",
      "  2: [1000/3022] 0.8777\n",
      "  2: [1100/3022] 0.8785\n",
      "  2: [1200/3022] 0.8776\n",
      "  2: [1300/3022] 0.8776\n",
      "  2: [1400/3022] 0.8777\n",
      "  2: [1500/3022] 0.8778\n",
      "  2: [1600/3022] 0.8778\n",
      "  2: [1700/3022] 0.8778\n",
      "  2: [1800/3022] 0.8779\n",
      "  2: [1900/3022] 0.8781\n",
      "  2: [2000/3022] 0.8782\n",
      "  2: [2100/3022] 0.8781\n",
      "  2: [2200/3022] 0.8780\n",
      "  2: [2300/3022] 0.8779\n",
      "  2: [2400/3022] 0.8777\n",
      "  2: [2500/3022] 0.8777\n",
      "  2: [2600/3022] 0.8776\n",
      "  2: [2700/3022] 0.8778\n",
      "  2: [2800/3022] 0.8780\n",
      "  2: [2900/3022] 0.8782\n",
      "  2: [3000/3022] 0.8781\n",
      "  2: [   0/ 755] 0.8125\n",
      "  2: [ 100/ 755] 0.8772\n",
      "  2: [ 200/ 755] 0.8762\n",
      "  2: [ 300/ 755] 0.8783\n",
      "  2: [ 400/ 755] 0.8791\n",
      "  2: [ 500/ 755] 0.8797\n",
      "  2: [ 600/ 755] 0.8806\n",
      "  2: [ 700/ 755] 0.8806\n",
      "754/755 validation\n",
      "epoch:2/5 train_accuracy:0.87815912475182 | val_accuracy:0.8805670529801325\n",
      "training\n",
      "  3: [   0/3022] 0.8750\n",
      "  3: [ 100/3022] 0.8769\n",
      "  3: [ 200/3022] 0.8808\n",
      "  3: [ 300/3022] 0.8795\n",
      "  3: [ 400/3022] 0.8814\n",
      "  3: [ 500/3022] 0.8810\n",
      "  3: [ 600/3022] 0.8806\n",
      "  3: [ 700/3022] 0.8798\n",
      "  3: [ 800/3022] 0.8803\n",
      "  3: [ 900/3022] 0.8804\n",
      "  3: [1000/3022] 0.8798\n",
      "  3: [1100/3022] 0.8800\n",
      "  3: [1200/3022] 0.8800\n",
      "  3: [1300/3022] 0.8797\n",
      "  3: [1400/3022] 0.8794\n",
      "  3: [1500/3022] 0.8793\n",
      "  3: [1600/3022] 0.8797\n",
      "  3: [1700/3022] 0.8798\n",
      "  3: [1800/3022] 0.8795\n",
      "  3: [1900/3022] 0.8795\n",
      "  3: [2000/3022] 0.8794\n",
      "  3: [2100/3022] 0.8794\n",
      "  3: [2200/3022] 0.8788\n",
      "  3: [2300/3022] 0.8787\n",
      "  3: [2400/3022] 0.8787\n",
      "  3: [2500/3022] 0.8787\n",
      "  3: [2600/3022] 0.8786\n",
      "  3: [2700/3022] 0.8784\n",
      "  3: [2800/3022] 0.8785\n",
      "  3: [2900/3022] 0.8785\n",
      "  3: [3000/3022] 0.8782\n",
      "  3: [   0/ 755] 0.8594\n",
      "  3: [ 100/ 755] 0.8772\n",
      "  3: [ 200/ 755] 0.8812\n",
      "  3: [ 300/ 755] 0.8823\n",
      "  3: [ 400/ 755] 0.8804\n",
      "  3: [ 500/ 755] 0.8816\n",
      "  3: [ 600/ 755] 0.8806\n",
      "  3: [ 700/ 755] 0.8812\n",
      "754/755 validation\n",
      "epoch:3/5 train_accuracy:0.8781436135009927 | val_accuracy:0.8805463576158941\n",
      "training\n",
      "  4: [   0/3022] 0.8125\n",
      "  4: [ 100/3022] 0.8730\n",
      "  4: [ 200/3022] 0.8744\n",
      "  4: [ 300/3022] 0.8781\n",
      "  4: [ 400/3022] 0.8783\n",
      "  4: [ 500/3022] 0.8782\n",
      "  4: [ 600/3022] 0.8779\n",
      "  4: [ 700/3022] 0.8767\n",
      "  4: [ 800/3022] 0.8762\n",
      "  4: [ 900/3022] 0.8763\n",
      "  4: [1000/3022] 0.8762\n",
      "  4: [1100/3022] 0.8766\n",
      "  4: [1200/3022] 0.8774\n",
      "  4: [1300/3022] 0.8777\n",
      "  4: [1400/3022] 0.8782\n",
      "  4: [1500/3022] 0.8782\n",
      "  4: [1600/3022] 0.8780\n",
      "  4: [1700/3022] 0.8780\n",
      "  4: [1800/3022] 0.8782\n",
      "  4: [1900/3022] 0.8780\n",
      "  4: [2000/3022] 0.8782\n",
      "  4: [2100/3022] 0.8780\n",
      "  4: [2200/3022] 0.8781\n",
      "  4: [2300/3022] 0.8780\n",
      "  4: [2400/3022] 0.8778\n",
      "  4: [2500/3022] 0.8780\n",
      "  4: [2600/3022] 0.8781\n",
      "  4: [2700/3022] 0.8782\n",
      "  4: [2800/3022] 0.8781\n",
      "  4: [2900/3022] 0.8781\n",
      "  4: [3000/3022] 0.8781\n",
      "  4: [   0/ 755] 0.8594\n",
      "  4: [ 100/ 755] 0.8815\n",
      "  4: [ 200/ 755] 0.8840\n",
      "  4: [ 300/ 755] 0.8794\n",
      "  4: [ 400/ 755] 0.8790\n",
      "  4: [ 500/ 755] 0.8787\n",
      "  4: [ 600/ 755] 0.8793\n",
      "  4: [ 700/ 755] 0.8799\n",
      "754/755 validation\n",
      "epoch:4/5 train_accuracy:0.8781539543348775 | val_accuracy:0.8805670529801325\n",
      "1821.5344359874725 secs\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "training(train_dataset,val_dataset,criterion,optim,epochs,model,epoch_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"./model/senti-attention.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
